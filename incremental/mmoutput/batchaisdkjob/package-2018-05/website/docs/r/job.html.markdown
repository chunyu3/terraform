---
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    AUTO GENERATED CODE     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file at
#     https://github.com/Azure/magic-module-specs
#
# ----------------------------------------------------------------------------
layout: "azurerm"
page_title: "Azure Resource Manager: azurerm_job"
sidebar_current: "docs-azurerm-resource-job"
description: |-
  Manage Azure Job instance.
---

# azurerm_job

Manage Azure Job instance.


## Argument Reference

The following arguments are supported:

* `name` - (Required) The name of the job within the specified resource group. Job names can only contain a combination of alphanumeric characters along with dash (-) and underscore (_). The name must be from 1 through 64 characters long. Changing this forces a new resource to be created.

* `resource_group` - (Required) Name of the resource group to which the resource belongs. Changing this forces a new resource to be created.

* `cluster` - (Required) One `cluster` block defined below.

* `experiment_name` - (Required) The name of the experiment. Experiment names can only contain a combination of alphanumeric characters along with dash (-) and underscore (_). The name must be from 1 through 64 characters long. Changing this forces a new resource to be created.

* `node_count` - (Required) Number of compute nodes to run the job on. The job will be gang scheduled on that many compute nodes.

* `std_out_err_path_prefix` - (Required) The path where the Batch AI service will store stdout, stderror and execution log of the job.

* `workspace_name` - (Required) The name of the workspace. Workspace names can only contain a combination of alphanumeric characters along with dash (-) and underscore (_). The name must be from 1 through 64 characters long. Changing this forces a new resource to be created.

---

The `cluster` block supports the following:

* `id` - (Required) The ID of the resource

* `caffe2settings` - (Optional) One `caffe2setting` block defined below.

* `caffe_settings` - (Optional) One `caffe_setting` block defined below.

* `chainer_settings` - (Optional) One `chainer_setting` block defined below.

* `cntk_settings` - (Optional) One `cntk_setting` block defined below.

* `constraints` - (Optional) One `constraint` block defined below.

* `container_settings` - (Optional) One `container_setting` block defined below.

* `custom_mpi_settings` - (Optional) One `custom_mpi_setting` block defined below.

* `custom_toolkit_settings` - (Optional) One `custom_toolkit_setting` block defined below.

* `environment_variables` - (Optional) One or more `environment_variable` block defined below.

* `horovod_settings` - (Optional) One `horovod_setting` block defined below.

* `input_directories` - (Optional) One or more `input_directory` block defined below.

* `job_preparation` - (Optional) One `job_preparation` block defined below.

* `mount_volumes` - (Optional) One `mount_volume` block defined below.

* `output_directories` - (Optional) One or more `output_directory` block defined below.

* `py_torch_settings` - (Optional) One `py_torch_setting` block defined below.

* `scheduling_priority` - (Optional) Scheduling priority associated with the job. Possible values: low, normal, high. Defaults to `low`.

* `secrets` - (Optional) One or more `secret` block defined below.

* `tensor_flow_settings` - (Optional) One `tensor_flow_setting` block defined below.

---

The `caffe2setting` block supports the following:

* `python_script_file_path` - (Required) The python script to execute.

* `python_interpreter_path` - (Optional) The path to the Python interpreter.

* `command_line_args` - (Optional) Command line arguments that need to be passed to the python script.

---

The `caffe_setting` block supports the following:

* `config_file_path` - (Optional) Path of the config file for the job. This property cannot be specified if pythonScriptFilePath is specified.

* `python_script_file_path` - (Optional) Python script to execute. This property cannot be specified if configFilePath is specified.

* `python_interpreter_path` - (Optional) The path to the Python interpreter. The property can be specified only if the pythonScriptFilePath is specified.

* `command_line_args` - (Optional) Command line arguments that need to be passed to the Caffe job.

* `process_count` - (Optional) Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property

---

The `chainer_setting` block supports the following:

* `python_script_file_path` - (Required) The python script to execute.

* `python_interpreter_path` - (Optional) The path to the Python interpreter.

* `command_line_args` - (Optional) Command line arguments that need to be passed to the python script.

* `process_count` - (Optional) Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property

---

The `cntk_setting` block supports the following:

* `language_type` - (Optional) The language to use for launching CNTK (aka Microsoft Cognitive Toolkit) job. Valid values are 'BrainScript' or 'Python'.

* `config_file_path` - (Optional) Specifies the path of the BrainScript config file. This property can be specified only if the languageType is 'BrainScript'.

* `python_script_file_path` - (Optional) Python script to execute. This property can be specified only if the languageType is 'Python'.

* `python_interpreter_path` - (Optional) The path to the Python interpreter. This property can be specified only if the languageType is 'Python'.

* `command_line_args` - (Optional) Command line arguments that need to be passed to the python script or cntk executable.

* `process_count` - (Optional) Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property

---

The `constraint` block supports the following:

* `max_wall_clock_time` - (Optional) Max time the job can run. Default value: 1 week.

---

The `container_setting` block supports the following:

* `image_source_registry` - (Required) One `image_source_registry` block defined below.

* `shm_size` - (Optional) Size of /dev/shm. Please refer to docker documentation for supported argument formats.


---

The `image_source_registry` block supports the following:

* `server_url` - (Optional) URL for image repository.

* `image` - (Required) The name of the image in the image repository.

* `credentials` - (Optional) One `credential` block defined below.


---

The `credential` block supports the following:

* `username` - (Required) User name to login to the repository.

* `password` - (Optional) User password to login to the docker repository. One of password or passwordSecretReference must be specified.

---

The `custom_mpi_setting` block supports the following:

* `command_line` - (Required) The command line to be executed by mpi runtime on each compute node.

* `process_count` - (Optional) Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property

---

The `custom_toolkit_setting` block supports the following:

* `command_line` - (Optional) The command line to execute on the master node.

---

The `environment_variable` block supports the following:

* `name` - (Required) The name of the environment variable.

* `value` - (Required) The value of the environment variable.

---

The `horovod_setting` block supports the following:

* `python_script_file_path` - (Required) The python script to execute.

* `python_interpreter_path` - (Optional) The path to the Python interpreter.

* `command_line_args` - (Optional) Command line arguments that need to be passed to the python script.

* `process_count` - (Optional) Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property

---

The `input_directory` block supports the following:

* `id` - (Required) The ID for the input directory. The job can use AZ_BATCHAI_INPUT_<id> environment variable to find the directory path, where <id> is the value of id attribute.

* `path` - (Required) The path to the input directory.

---

The `job_preparation` block supports the following:

* `command_line` - (Required) The command line to execute. If containerSettings is specified on the job, this commandLine will be executed in the same container as job. Otherwise it will be executed on the node.

---

The `mount_volume` block supports the following:

* `azure_file_shares` - (Optional) One or more `azure_file_share` block defined below.

* `azure_blob_file_systems` - (Optional) One or more `azure_blob_file_system` block defined below.

* `file_servers` - (Optional) One or more `file_server` block defined below.

* `unmanaged_file_systems` - (Optional) One or more `unmanaged_file_system` block defined below.


---

The `azure_file_share` block supports the following:

* `account_name` - (Required) Name of the Azure storage account.

* `azure_file_url` - (Required) URL to access the Azure File.

* `credentials` - (Required) One `credential` block defined below.

* `relative_mount_path` - (Required) The relative path on the compute node where the Azure File share will be mounted. Note that all cluster level file shares will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level file shares will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

* `file_mode` - (Optional) File mode for files on the mounted file share. Default value: 0777.

* `directory_mode` - (Optional) File mode for directories on the mounted file share. Default value: 0777.


---

The `credential` block supports the following:

* `account_key` - (Optional) Storage account key. One of accountKey or accountKeySecretReference must be specified.

---

The `azure_blob_file_system` block supports the following:

* `account_name` - (Required) Name of the Azure storage account.

* `container_name` - (Required) Name of the Azure Blob Storage container to mount on the cluster.

* `credentials` - (Required) One `credential` block defined below.

* `relative_mount_path` - (Required) The relative path on the compute node where the Azure File container will be mounted. Note that all cluster level containers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level containers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

* `mount_options` - (Optional) Mount options for mounting blobfuse file system.


---

The `credential` block supports the following:

* `account_key` - (Optional) Storage account key. One of accountKey or accountKeySecretReference must be specified.

---

The `file_server` block supports the following:

* `file_server` - (Required) One `file_server` block defined below.

* `source_directory` - (Optional) File Server directory that needs to be mounted. If this property is not specified, the entire File Server will be mounted.

* `relative_mount_path` - (Required) The relative path on the compute node where the File Server will be mounted. Note that all cluster level file servers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level file servers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

* `mount_options` - (Optional) Mount options to be passed to mount command.


---

The `file_server` block supports the following:

* `id` - (Required) The ID of the resource

---

The `unmanaged_file_system` block supports the following:

* `mount_command` - (Required) Mount command line. Note, Batch AI will append mount path to the command on its own.

* `relative_mount_path` - (Required) The relative path on the compute node where the unmanaged file system will be mounted. Note that all cluster level unmanaged file systems will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level unmanaged file systems will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.

---

The `output_directory` block supports the following:

* `id` - (Required) The ID of the output directory. The job can use AZ_BATCHAI_OUTPUT_<id> environment variable to find the directory path, where <id> is the value of id attribute.

* `path_prefix` - (Required) The prefix path where the output directory will be created. Note, this is an absolute path to prefix. E.g. $AZ_BATCHAI_MOUNT_ROOT/MyNFS/MyLogs. The full path to the output directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.

* `path_suffix` - (Optional) The suffix path where the output directory will be created. E.g. models. You can find the full path to the output directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.

---

The `py_torch_setting` block supports the following:

* `python_script_file_path` - (Required) The python script to execute.

* `python_interpreter_path` - (Optional) The path to the Python interpreter.

* `command_line_args` - (Optional) Command line arguments that need to be passed to the python script.

* `process_count` - (Optional) Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property

* `communication_backend` - (Optional) Type of the communication backend for distributed jobs. Valid values are 'TCP', 'Gloo' or 'MPI'. Not required for non-distributed jobs.

---

The `secret` block supports the following:

* `name` - (Required) The name of the environment variable to store the secret value.

* `value` - (Optional) The value of the environment variable. This value will never be reported back by Batch AI.

* `value_secret_reference` - (Optional) One `value_secret_reference` block defined below.


---

The `value_secret_reference` block supports the following:

* `source_vault` - (Required) One `source_vault` block defined below.

* `secret_url` - (Required) The URL referencing a secret in the Key Vault.


---

The `source_vault` block supports the following:

* `id` - (Required) The ID of the resource

---

The `tensor_flow_setting` block supports the following:

* `python_script_file_path` - (Required) The python script to execute.

* `python_interpreter_path` - (Optional) The path to the Python interpreter.

* `master_command_line_args` - (Optional) Command line arguments that need to be passed to the python script for the master task.

* `worker_command_line_args` - (Optional) Command line arguments that need to be passed to the python script for the worker task. Optional for single process jobs.

* `parameter_server_command_line_args` - (Optional) Command line arguments that need to be passed to the python script for the parameter server. Optional for single process jobs.

* `worker_count` - (Optional) The number of worker tasks. If specified, the value must be less than or equal to (nodeCount * numberOfGPUs per VM). If not specified, the default value is equal to nodeCount. This property can be specified only for distributed TensorFlow training.

* `parameter_server_count` - (Optional) The number of parameter server tasks. If specified, the value must be less than or equal to nodeCount. If not specified, the default value is equal to 1 for distributed TensorFlow training. This property can be specified only for distributed TensorFlow training.

## Attributes Reference

The following attributes are exported:

* `tool_type` - Possible values are: cntk, tensorflow, caffe, caffe2, chainer, pytorch, custom, custommpi, horovod.

* `job_output_directory_path_segment` - A segment of job's output directories path created by Batch AI. Batch AI creates job's output directories under an unique path to avoid conflicts between jobs. This value contains a path segment generated by Batch AI to make the path unique and can be used to find the output directory on the node or mounted filesystem.

* `creation_time` - The creation time of the job.

* `provisioning_state` - The provisioned state of the Batch AI job

* `provisioning_state_transition_time` - The time at which the job entered its current provisioning state.

* `execution_state` - The current state of the job. Possible values are: queued - The job is queued and able to run. A job enters this state when it is created, or when it is awaiting a retry after a failed run. running - The job is running on a compute cluster. This includes job-level preparation such as downloading resource files or set up container specified on the job - it does not necessarily mean that the job command line has started executing. terminating - The job is terminated by the user, the terminate operation is in progress. succeeded - The job has completed running successfully and exited with exit code 0. failed - The job has finished unsuccessfully (failed with a non-zero exit code) and has exhausted its retry limit. A job is also marked as failed if an error occurred launching the job.

* `execution_state_transition_time` - The time at which the job entered its current execution state.

* `execution_info` - One `execution_info` block defined below.

* `id` - The ID of the resource.

* `name` - The name of the resource.

* `type` - The type of the resource.


---

The `execution_info` block contains the following:

* `start_time` - (Optional) The time at which the job started running. 'Running' corresponds to the running state. If the job has been restarted or retried, this is the most recent time at which the job started running. This property is present only for job that are in the running or completed state.

* `end_time` - (Optional) The time at which the job completed. This property is only returned if the job is in completed state.

* `exit_code` - (Optional) The exit code of the job. This property is only returned if the job is in completed state.

* `errors` - (Optional) One or more `error` block defined below.


---

The `error` block supports the following:

* `code` - (Optional) An identifier of the error. Codes are invariant and are intended to be consumed programmatically.

* `message` - (Optional) A message describing the error, intended to be suitable for display in a user interface.

* `details` - (Optional) One or more `detail` block defined below.


---

The `detail` block supports the following:

* `name` - (Optional) The name in the name-value pair.

* `value` - (Optional) The value in the name-value pair.
